{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132936aa-e507-4c85-8452-c65a16dfcaee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a2ee7-0e1d-4839-b99b-f8c88daa1846",
   "metadata": {
    "tags": []
   },
   "source": [
    "***The tasks of this assignment are to implement an improved version of the Naive Bayes algorithm that is able to predict the category of points of interest from the Yelp dataset - one of \"Restaurants\", \"Shopping\" and \"Nightlife\". Then apply the implementation on the test set without class labels and submit the predictions to Kaggle, the system will evaluate the results with the ground-truth data and report the accuracy.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555a242-e152-4a1e-9ac9-ef94f614d58f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Report Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14326a4c-d73f-45e1-a483-b3bd50c6a2c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b663b6-b4d3-4b52-829e-1bf2270b65cc",
   "metadata": {},
   "source": [
    "The training set and test set of this assignment are separate files, which means unlike in the previous experimental environment, we cannot evaluate the model locally on the test set.</br>\n",
    "Two split procedures are considered when splitting the training set: training/validation split by 80/20, and 5-fold cross-validation.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effd464-86c9-47f3-87ef-e5d505d46d7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4d332-db5a-4bf4-8c37-9026b8dfc7ab",
   "metadata": {},
   "source": [
    "After loading the data and inspection, the integrity of the dataset is good with no missing values.</br>\n",
    "But the tasks of this assignment are to classify text data, so it is necessary to do some preprocessing on the text data before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1616a7c-3bd3-44bc-8c3a-bb48b928cd9e",
   "metadata": {},
   "source": [
    "#### 2.1 Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9aa369-6d83-42ff-9a4f-16488d811fbf",
   "metadata": {},
   "source": [
    "First we define some preprocessing rules, including: convert letters to lowercase; remove numeric characters; remove punctuation.</br>\n",
    "These rules will be applied as a preprocessor in the next feature extraction stage to remove some unnecessary noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b8760-7bbb-49e7-a18f-15e7f07edcbe",
   "metadata": {},
   "source": [
    "#### 2.2 Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8beeece-47aa-40a3-9aa9-8b3a8fb3f8c2",
   "metadata": {},
   "source": [
    "Sklearn provides vectorizers such as CountVectorizer and TfidfVectorizer etc., both of which are common text feature extraction methods.</br>\n",
    "For each text data, CountVectorizer only considers the frequency of each word appearing in the text.</br>\n",
    "While TfidfVectorizer is based on CountVectorizer and also pays attention to the inverse of the number of other texts that contain this word.</br>\n",
    "In contrast, the larger the number of training sets, the more advantageous the feature quantization method TfidfVectorizer is.</br>\n",
    "In this assignment, due to the limited number of training sets, we choose CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8db5c5-ed21-4dd4-b835-a95be75a87f0",
   "metadata": {},
   "source": [
    "#### 2.3 Parameters of CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97cb87-6139-4c72-8c6c-d14309d4af3b",
   "metadata": {},
   "source": [
    "CountVectorizer has several parameters that can be adjusted.</br>\n",
    "Setting stop words can filter out some frequently occurring but meaningless words, such as articles, prepositions or conjunctions.</br>\n",
    "Setting token_pattern can filter out one- or two-letter words that interfere with training.</br>\n",
    "The larger the max_features, the higher the accuracy of the model, but too large max_features may lead to a decrease in the generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcdff69-c0db-471b-a37b-7e2882b0b29e",
   "metadata": {},
   "source": [
    "### 3. Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572811d-d75d-4982-8e43-8640bdbe84ad",
   "metadata": {},
   "source": [
    "Sklearn provides Naive Bayesian models such as BernoulliNB, GaussianNB, MultinomialNB etc.</br>\n",
    "The MultinomialNB is suitable for classification with discrete features such as text classification.</br>\n",
    "The multinomial distribution normally requires integer feature counts, so CountVectorizer and MultinomialNB are good combination for this assignment.</br>\n",
    "The parameters of MultinomialNB include: smoothing parameter 'alpha', 'fit_prior' and 'class_prior'.</br>\n",
    "We use GridSearchCV to tune 'alpha' and 'fit_prior' to get the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165ac83-3700-47a1-96b0-9852a79d1e3e",
   "metadata": {},
   "source": [
    "### 4. Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de5ccc-3742-4db9-bb5e-1cefeb00eaaa",
   "metadata": {},
   "source": [
    "#### 4.1 Task_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924c91c-804f-4733-84fd-9758aaa82263",
   "metadata": {},
   "source": [
    "In task 1, we train the model separately with the above two procedures while keeping the model parameters unchanged.</br>\n",
    "The results show that the performance of the 5-fold cross-validation method is better than that of training/validation split by 80 /20.</br>\n",
    "So in task 2, we only use 5-fold cross-validation to train the model.</br>\n",
    "As requested, we only train the model based on the \"review\" attribute in task 1.</br>\n",
    "While we use \"predict_proba\" to preserve the probabilities of the test results to facilitate combining with other naive Bayesian models in task 2.</br>\n",
    "In task 1, we get two outputs: \"predict_r_s.csv\" and \"predict_R_cv.csv\" from the training/validation split and 5-fold cross-validation models, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815091cc-e16d-489c-bb39-46ae7946aa63",
   "metadata": {},
   "source": [
    "#### 4.2 Task_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087dd1c-18a1-491f-a8b6-59e4bcf345f3",
   "metadata": {},
   "source": [
    "In Task 2, we consider that attributes \"name\" and \"mean_checkin_time\" may be useful for the prediction.</br>\n",
    "First we train the model based only on the \"name\" attribute, and then get the probability of the test data under this model via \"predict_proba\".</br>\n",
    "According to the Naive Bayes theorem, the attributes are independent of each other, so the largest p(y|x1,x2) can be calculated by multiplying the \"predict_proba\" of the two models and dividing by p(y).</br>\n",
    "Similarly, We retrain the model only based on the \"mean_checkin_time\" attribute and get the test probability under that model.</br>\n",
    "Next, we combine the conditional probabilities obtained in:</br>\n",
    "Task 1 Section 1.3 (ie, the model based only on the \"review\" attribute and cross-validation) with</br>\n",
    "Task 2 Section 2.2 (ie, the model based only on the \"name\" attribute), to get the Prediction result \"predict_R_N.csv\".</br>\n",
    "Similarly, we combine \"predict_R_N.csv\" with</br>\n",
    "Task 2 Section 2.3 (i.e. the model based only on the \"time\" attribute) to get the prediction result \"predict_R_N_T.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05198ea-a95e-4e06-b628-585600506184",
   "metadata": {},
   "source": [
    "### 5. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5f4b9-ca65-46f5-b8cd-06d108c4f5b9",
   "metadata": {},
   "source": [
    "Task 1 Section 1.2: The validation set running on the model gets an accuracy of 0.89043.</br>\n",
    "We submit the prediction result 'predict_r_s.csv' to Kaggle and get a Public Score of 0.87572, slightly higher than the baseline of 0.87283.</br>\n",
    "Task 1 Section 1.3: The validation set running on the model gets a mean accuracy of 0.89001.</br>\n",
    "We submit the prediction result 'predict_R_cv.csv' to Kaggle and get a Public Score of 0.87861, slightly higher than 'predict_1.csv' of 0.87572 and the baseline of 0.87283.</br>\n",
    "Task 2 Section 2.2: We train the model only based on the \"name\" attribute with cross-validation, and the validation accuracy is not high (0.75844).</br>\n",
    "However, after we combine the model trained with cross-validation in task 1, the accuracy of the prediction results is greatly improved.</br>\n",
    "We submit the prediction result 'predict_R_N.csv' to Kaggle and get a Public Score of 0.90462, higher than the baseline of 0.87283.</br>\n",
    "Task 2 Section 2.3: We train the model only based on the \"time\" attribute with cross-validation, and the validation accuracy is very low (0.61922).</br>\n",
    "After we combine 'predict_R_N.csv', the accuracy of the prediction results is not changed.</br>\n",
    "We submit the prediction result 'predict_R_N_T.csv' to Kaggle and get a Public Score of 0.90462, same like 'predict_R_N.csv', higher than the baseline of 0.87283."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63868128-a871-4e40-aa58-4315e7a4cd87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102169f8-4a1f-4b8f-9e0c-7cd00c30997a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5a503-8164-4c7c-8853-dbf97951ab59",
   "metadata": {},
   "source": [
    "***Before starting, we need to import the relevant packages and set the random seed.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86c389e-22a3-4731-9b06-b04d4b06f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4440eec2-b0f2-4637-b54a-6555b3320714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload local python files every 2 seconds\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3854a55d-34ac-4f94-bf37-5bfc14c7c692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "RANDOM_STATE = 1234\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b1b2c-57f9-40ce-8de6-407938bdb183",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3129b5c-886a-4c4f-8e4d-e50da3429977",
   "metadata": {},
   "source": [
    "***Improve the benchmark model based on the review attribute only. Since it is a text classification problem, we will consider \"MultinomialNB\" as the classifier, and \"CountVectorizer\" as the text preprocessor.</br>\n",
    "In addition, two procedures for splitting the training set will be tried separately: splitting the training and validation sets by 80/20, and splitting the entire training set with 5-fold cross-validation.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432fcff-e926-4ab1-8fbf-32e31ef0be58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b042a2-5cab-40e0-976c-750becb120c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1.1 Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c99d5b2-ff2e-443a-bf07-844e4eaf0c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>mean_checkin_time</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3007</td>\n",
       "      <td>The New Orleans Vampire Cafe</td>\n",
       "      <td>29.959033</td>\n",
       "      <td>-90.064036</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Amazing service. Cool vibe. It's not spooky or...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1829</td>\n",
       "      <td>Ted's Frostop</td>\n",
       "      <td>29.947026</td>\n",
       "      <td>-90.113604</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Breakfast here is great and there's never a hu...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>298</td>\n",
       "      <td>The Will &amp; The Way</td>\n",
       "      <td>29.957573</td>\n",
       "      <td>-90.065827</td>\n",
       "      <td>9.5</td>\n",
       "      <td>So glad that we stumbled in here! The cheesebu...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245</td>\n",
       "      <td>Public Belt</td>\n",
       "      <td>29.946393</td>\n",
       "      <td>-90.063729</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AMAZING! Try this place out.  Great for some g...</td>\n",
       "      <td>Nightlife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2902</td>\n",
       "      <td>Phillys Cafe</td>\n",
       "      <td>29.941818</td>\n",
       "      <td>-90.094797</td>\n",
       "      <td>18.0</td>\n",
       "      <td>WooHoo, best philly cheese staks I have had in...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                          name   latitude  longitude  \\\n",
       "0  3007  The New Orleans Vampire Cafe  29.959033 -90.064036   \n",
       "1  1829                 Ted's Frostop  29.947026 -90.113604   \n",
       "2   298            The Will & The Way  29.957573 -90.065827   \n",
       "3  1245                   Public Belt  29.946393 -90.063729   \n",
       "4  2902                  Phillys Cafe  29.941818 -90.094797   \n",
       "\n",
       "   mean_checkin_time                                             review  \\\n",
       "0               17.0  Amazing service. Cool vibe. It's not spooky or...   \n",
       "1               17.0  Breakfast here is great and there's never a hu...   \n",
       "2                9.5  So glad that we stumbled in here! The cheesebu...   \n",
       "3                3.0  AMAZING! Try this place out.  Great for some g...   \n",
       "4               18.0  WooHoo, best philly cheese staks I have had in...   \n",
       "\n",
       "      category  \n",
       "0  Restaurants  \n",
       "1  Restaurants  \n",
       "2  Restaurants  \n",
       "3    Nightlife  \n",
       "4  Restaurants  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and check integrity\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20023f72-67cf-4edb-b271-49928dbdc7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2873 entries, 0 to 2872\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   ID                 2873 non-null   int64  \n",
      " 1   name               2873 non-null   object \n",
      " 2   latitude           2873 non-null   float64\n",
      " 3   longitude          2873 non-null   float64\n",
      " 4   mean_checkin_time  2873 non-null   float64\n",
      " 5   review             2873 non-null   object \n",
      " 6   category           2873 non-null   object \n",
      "dtypes: float64(3), int64(1), object(3)\n",
      "memory usage: 157.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a8529e-a43c-4830-83df-2706f7fc5749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                   0\n",
       "name                 0\n",
       "latitude             0\n",
       "longitude            0\n",
       "mean_checkin_time    0\n",
       "review               0\n",
       "category             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e660bb00-082c-45ab-b912-a740c4e1b5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Restaurants    1779\n",
       "Shopping        782\n",
       "Nightlife       312\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c1109-2982-4c64-8786-4bcd39fbb036",
   "metadata": {},
   "source": [
    "***The integrity of the training set is good, no missing values are found. But in these three categories, \"restaurants\" has the largest proportion, while \"nightlife\" has the smallest proportion.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52390e6c-2a31-479d-b375-24221a768bf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1.2 Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8511c49-8fdb-4231-82f0-71b6d712be8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>mean_checkin_time</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2406</td>\n",
       "      <td>Courtyard Grill Restaurant at Bourbon Heat</td>\n",
       "      <td>29.958905</td>\n",
       "      <td>-90.065500</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Terrible food and service . don't waste your m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1401</td>\n",
       "      <td>Papa John's Pizza</td>\n",
       "      <td>29.944986</td>\n",
       "      <td>-90.076830</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Like everyone else said, don't order if you're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2783</td>\n",
       "      <td>District Donut &amp; Coffee Bar</td>\n",
       "      <td>29.921412</td>\n",
       "      <td>-90.117817</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Great little breakfast spot!   The donuts are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1352</td>\n",
       "      <td>Pyramids Cafe</td>\n",
       "      <td>29.947334</td>\n",
       "      <td>-90.113001</td>\n",
       "      <td>17.0</td>\n",
       "      <td>This place gets 4 stars for service, delicious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303</td>\n",
       "      <td>Tsunami Sushi</td>\n",
       "      <td>29.949966</td>\n",
       "      <td>-90.069677</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Pleasantly surprised by the presentation of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                        name   latitude  longitude  \\\n",
       "0  2406  Courtyard Grill Restaurant at Bourbon Heat  29.958905 -90.065500   \n",
       "1  1401                           Papa John's Pizza  29.944986 -90.076830   \n",
       "2  2783                 District Donut & Coffee Bar  29.921412 -90.117817   \n",
       "3  1352                               Pyramids Cafe  29.947334 -90.113001   \n",
       "4   303                               Tsunami Sushi  29.949966 -90.069677   \n",
       "\n",
       "   mean_checkin_time                                             review  \n",
       "0                7.5  Terrible food and service . don't waste your m...  \n",
       "1               18.0  Like everyone else said, don't order if you're...  \n",
       "2               16.0  Great little breakfast spot!   The donuts are ...  \n",
       "3               17.0  This place gets 4 stars for service, delicious...  \n",
       "4               14.0  Pleasantly surprised by the presentation of th...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and check integrity\n",
    "test = pd.read_csv('test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0225ebc-ab5b-4834-a1d2-63cc4a7ffbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 693 entries, 0 to 692\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   ID                 693 non-null    int64  \n",
      " 1   name               693 non-null    object \n",
      " 2   latitude           693 non-null    float64\n",
      " 3   longitude          693 non-null    float64\n",
      " 4   mean_checkin_time  693 non-null    float64\n",
      " 5   review             693 non-null    object \n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 32.6+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47706fba-7cde-49fe-9234-64cc89784700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                   0\n",
       "name                 0\n",
       "latitude             0\n",
       "longitude            0\n",
       "mean_checkin_time    0\n",
       "review               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7a0d4-6837-4877-9875-c120c94775d1",
   "metadata": {},
   "source": [
    "***The integrity of the testing set is good, no missing values are found.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a043da4-38b9-4f13-99b2-40d611bee66d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Method 1 (Split dataset by 80/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681772c5-e482-469e-a905-ce0508b48392",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2.1 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "046a03be-a13c-4be3-8361-1f9d73c98a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2873,) (2873,) (693,)\n"
     ]
    }
   ],
   "source": [
    "# Split features and class\n",
    "x_r = train['review'].to_numpy()\n",
    "y_r = train['category'].to_numpy()\n",
    "x_test_r = test['review'].to_numpy()\n",
    "print(x_r.shape, y_r.shape, x_test_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a253f95-976e-46e9-b800-d7e4b712f052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2298,) (575,) (2298,) (575,)\n"
     ]
    }
   ],
   "source": [
    "x_train_r, x_val_r, y_train_r, y_val_r = train_test_split(x_r, y_r, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(x_train_r.shape, x_val_r.shape, y_train_r.shape, y_val_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75558f-8943-4fb7-8ade-deef3522dd60",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2.2 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d0fa120-d7b4-43e6-9f4b-767f2450da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing rules\n",
    "def preprocessor_r(text):\n",
    "    text = text.lower() # convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text) # remove numeric characters\n",
    "    text = re.sub(r'\\W+', ' ', text) # remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e5251ab-e79b-48c4-8e1b-e9edc9964595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2298, 5000) (575, 5000)\n"
     ]
    }
   ],
   "source": [
    "cv_r = CountVectorizer(preprocessor=preprocessor_r, stop_words='english', token_pattern=r'(?u)\\b\\w\\w\\w+\\b', max_features=5000)\n",
    "x_train_cv_r = cv_r.fit_transform(x_train_r)\n",
    "x_val_cv_r = cv_r.transform(x_val_r)\n",
    "print(x_train_cv_r.shape, x_val_cv_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d02544-b2f4-4715-8d85-4fe41ea4b401",
   "metadata": {},
   "source": [
    "***Before training the model, we need to preprocess the text data, remove unnecessary noise by setting some preprocessing rules.</br>\n",
    "Then vectorize the data by using \"CountVectorizer\":</br>\n",
    "Setting stop words can filter out some frequently occurring but meaningless words, such as articles, prepositions or conjunctions.</br>\n",
    "Setting token_pattern can filter out one- or two-letter words that interfere with training.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92681dac-598b-41a8-8c95-a5e43feb62d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9e09b77-4cef-4502-8fab-6090a3d99e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.986)\n"
     ]
    }
   ],
   "source": [
    "estimator_r = MultinomialNB()\n",
    "param_grid_r = {'alpha': np.arange(0, 1, 0.001), 'fit_prior': [True, False]}\n",
    "grid_search_r = GridSearchCV(estimator=estimator_r, param_grid=param_grid_r, cv=10, n_jobs=-1)\n",
    "grid_search_r.fit(x_train_cv_r, y_train_r)\n",
    "best_estimator_r = grid_search_r.best_estimator_\n",
    "best_estimator_r.fit(x_train_cv_r, y_train_r)\n",
    "print(best_estimator_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e05bc-403b-4872-8c28-fe97d17fbb86",
   "metadata": {},
   "source": [
    "***The parameters of MultinomialNB include smoothing parameter alpha, fit_prior and class_prior.\n",
    "We use GridSearchCV to tune some parameters to get the best model.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aea9ee-b90e-4e5b-b2df-7db02749b9d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2.4 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a97665fe-b199-4949-b0d2-a54d758dced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8904347826086957\n"
     ]
    }
   ],
   "source": [
    "score_r = best_estimator_r.score(x_val_cv_r, y_val_r)\n",
    "print('Accuracy:', score_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef08b5e-7f11-4eb2-854c-094216bc2d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2.5 Predict the test data and output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4218e1e8-8447-4375-a0b4-f9ab82b296f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "x_test_cv_r = cv_r.transform(x_test_r)\n",
    "y_test_r = best_estimator_r.predict(x_test_cv_r)\n",
    "proba_test_r = best_estimator_r.predict_proba(x_test_cv_r)\n",
    "\n",
    "# Output\n",
    "result_r = zip((test['ID']), y_test_r)\n",
    "output_r = pd.DataFrame(data=result_r, columns=['ID', 'category'])\n",
    "output_r.to_csv('predict_r_s.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb67d53-43d9-415d-b95e-a856a601c558",
   "metadata": {},
   "source": [
    "***The validation set running on the model gets an accuracy of 0.89043.\n",
    "We submit the prediction result 'predict_r_s.csv' to Kaggle and get a Public Score of 0.87572, slightly higher than the baseline of 0.87283.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6d41b-670c-446c-8219-85a3ceb6d934",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3 Method 2 (Split dataset with 5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e71cbd0-0714-4db9-9ffa-50a0c146986b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2873,) (2873,) (693,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "x_R = train['review'].to_numpy()\n",
    "y_R = train['category'].to_numpy()\n",
    "x_test_R = test['review'].to_numpy()\n",
    "print(x_R.shape, y_R.shape, x_test_R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "612296c7-0b73-4e9a-9bd0-0156ca2ecbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8900087865474928\n"
     ]
    }
   ],
   "source": [
    "# Create list to store the results of each evaluation\n",
    "scoreList_R = []\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)\n",
    "for train_index, val_index in kf.split(x_R, y_R):\n",
    "    x_train_R, x_val_R = x_R[train_index], x_R[val_index]\n",
    "    y_train_R, y_val_R = y_R[train_index], y_R[val_index]\n",
    "    \n",
    "    # Vectorization\n",
    "    cv_R = CountVectorizer(lowercase=True, preprocessor=preprocessor_r, stop_words='english', token_pattern=r'(?u)\\b\\w\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=5000)\n",
    "    x_train_cv_R = cv_R.fit_transform(x_train_R)\n",
    "    x_val_cv_R = cv_R.transform(x_val_R)\n",
    "    \n",
    "    # Train the model\n",
    "    estimator_R = MultinomialNB()\n",
    "    param_grid_R = {'alpha': np.arange(0, 1, 0.001), 'fit_prior': [True, False]}\n",
    "    grid_search_R = GridSearchCV(estimator=estimator_R, param_grid=param_grid_R, cv=10, n_jobs=-1)\n",
    "    grid_search_R.fit(x_train_cv_R, y_train_R)\n",
    "    best_estimator_R = grid_search_R.best_estimator_\n",
    "    best_estimator_R.fit(x_train_cv_R, y_train_R)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score_R = best_estimator_R.score(x_val_cv_R, y_val_R)\n",
    "    scoreList_R.append(score_R)\n",
    "\n",
    "print('Mean accuracy:', np.mean(scoreList_R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3838a8b-8ca6-47fd-aa73-5691be81b4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "x_test_cv_R = cv_R.transform(x_test_R)\n",
    "y_test_R = best_estimator_R.predict(x_test_cv_R)\n",
    "proba_test_R = best_estimator_R.predict_proba(x_test_cv_R)\n",
    "\n",
    "# Output\n",
    "result_R = zip((test['ID']), y_test_R)\n",
    "output_R = pd.DataFrame(data=result_R, columns=['ID', 'category'])\n",
    "output_R.to_csv('predict_R_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66df9e8-f1ab-4c44-8485-63fcf857c430",
   "metadata": {},
   "source": [
    "***The validation set running on the model gets a mean accuracy of 0.89001.\n",
    "We submit the prediction result 'predict_R_cv.csv' to Kaggle and get a Public Score of 0.87861, slightly higher than 'predict_1.csv' of 0.87572 and the baseline of 0.87283.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23052b87-ede1-4556-acaf-cda756645a0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f081345-829b-4fc5-8afc-1b28711a9f4e",
   "metadata": {},
   "source": [
    "***From the above data, it can be seen that when the parameters of MultinomialNB and CountVectorizer are unchanged, the model trained by cross-validation has better performance, although it is not very significant.\n",
    "It shows that the cross-validation method has a positive effect on the model training, but it also means that the computational cost is increased.\n",
    "The above two sets of prediction results are better than the baseline, so we start task 2.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849e875-6fcc-4764-bff6-57f88c9d46d1",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9448490-4498-4342-ac2e-e8c26c2e7f84",
   "metadata": {},
   "source": [
    "***Improve your model by adding additional attributes to your model.\n",
    "Based on the study of the training set, we consider that attributes \"name\" and \"mean_checkin_time\" may be useful for the prediction.\n",
    "In the following tasks, we will add them separately and test the prediction results.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a356011-67b0-4d66-9ffb-62ead1c72609",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "073e1289-69f0-4c63-8689-5737947c6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3dc93-7730-4390-8c24-4488f62093c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Base on the \"name\" attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c55306e-b99f-44f2-9a5e-d5c77361ecd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db89b6b8-6c00-46cf-bcf3-10e4eca575d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2873,) (2873,) (693,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "x_N = train['name'].to_numpy()\n",
    "y_N = train['category'].to_numpy()\n",
    "x_test_N = test['name'].to_numpy()\n",
    "print(x_N.shape, y_N.shape, x_test_N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce617d29-0d2b-4f79-b4dd-93c847d0f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing rules\n",
    "def preprocessor_n(text):\n",
    "    text = text.lower() # convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text) # remove numeric characters\n",
    "    text = re.sub(r'\\W+', ' ', text) # remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02e154bb-21b9-48c9-bcaf-f1e23105cfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7584438721405847\n"
     ]
    }
   ],
   "source": [
    "# Create list to store the results of each evaluation\n",
    "scoreList_N = []\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)\n",
    "for train_index, val_index in kf.split(x_N, y_N):\n",
    "    x_train_N, x_val_N = x_N[train_index], x_N[val_index]\n",
    "    y_train_N, y_val_N = y_N[train_index], y_N[val_index]\n",
    "    \n",
    "    # Vectorization\n",
    "    cv_N = CountVectorizer(lowercase=True, preprocessor=preprocessor_n, stop_words='english', token_pattern=r'(?u)\\b\\w\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=5000)\n",
    "    x_train_cv_N = cv_N.fit_transform(x_train_N)\n",
    "    x_val_cv_N = cv_N.transform(x_val_N)\n",
    "    \n",
    "    # Train the model\n",
    "    estimator_N = MultinomialNB()\n",
    "    param_grid_N = {'alpha': np.arange(0, 1, 0.001), 'fit_prior': [True, False]}\n",
    "    grid_search_N = GridSearchCV(estimator=estimator_N, param_grid=param_grid_N, cv=10, n_jobs=-1)\n",
    "    grid_search_N.fit(x_train_cv_N, y_train_N)\n",
    "    best_estimator_N = grid_search_N.best_estimator_\n",
    "    best_estimator_N.fit(x_train_cv_N, y_train_N)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score_N = best_estimator_N.score(x_val_cv_N, y_val_N)\n",
    "    scoreList_N.append(score_N)\n",
    "\n",
    "print('Mean accuracy:', np.mean(scoreList_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18ad61-3dc8-459f-afe9-aa36c824c0a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.2 Predict the probability of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "532df997-230d-47c9-ac91-c09c3886b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "x_test_cv_N = cv_N.transform(x_test_N)\n",
    "y_test_N = best_estimator_N.predict(x_test_cv_N)\n",
    "proba_test_N = best_estimator_N.predict_proba(x_test_cv_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff47a51-ce06-4dec-99fa-385b46be37d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.3 Combine the results of 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63e74c68-a33c-4502-8adc-75c73a656afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_R_N = (proba_test_R*proba_test_N)/[312/2873, 1779/2873, 782/2873]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afb21326-63e3-4125-9e9b-5a3cca9ee093",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_R_N = []\n",
    "for i in range(len(p_R_N)):\n",
    "    x = np.argmax(p_R_N[i])\n",
    "    if x == 0:\n",
    "        x = 'Nightlife'\n",
    "    elif x == 1:\n",
    "        x = 'Restaurants'\n",
    "    else:\n",
    "        x = 'Shopping'\n",
    "    l_R_N.append(x)\n",
    "l_R_N = np.array(l_R_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937ac83-84c6-434c-8b6c-1e53453ecd6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.4 Output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7fd05dc-ef92-4c92-bce9-d72036a92321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "result_R_N = zip((test['ID']), l_R_N)\n",
    "output_R_N = pd.DataFrame(data=result_R_N, columns=['ID', 'category'])\n",
    "output_R_N.to_csv('predict_R_N.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e0c86-63d2-472c-bcd2-900495caa689",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0631be-0582-47c6-9559-dcd2c17ca0fb",
   "metadata": {},
   "source": [
    "***We train the model only based on the \"name\" attribute with cross-validation, and the validation accuracy is not high (0.75844).</br>\n",
    "However, after we combine the model trained with cross-validation in task 1, the accuracy of the prediction results is greatly improved.</br>\n",
    "We submit the prediction result 'predict_R_N.csv' to Kaggle and get a Public Score of 0.90462, higher than the baseline of 0.87283.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c6732-6814-4a02-b846-385addd3abe0",
   "metadata": {},
   "source": [
    "### 2.3 Base on the \"mean_checkin_time\" attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792e4de-3e10-4ced-a52d-0c8b9cba4871",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.3.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc3a1c25-58a7-4870-921c-fe26ac8fd838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2873,) (2873,) (693,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "x_T = train['mean_checkin_time'].to_numpy()\n",
    "y_T = train['category'].to_numpy()\n",
    "x_test_T = test['mean_checkin_time'].to_numpy()\n",
    "print(x_T.shape, y_T.shape, x_test_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f594073-5a16-41cc-9730-8d41c23fbd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.6192219360702924\n"
     ]
    }
   ],
   "source": [
    "# Create list to store the results of each evaluation\n",
    "scoreList_T = []\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)\n",
    "for train_index, val_index in kf.split(x_T, y_T):\n",
    "    x_train_T, x_val_T = x_T[train_index], x_T[val_index]\n",
    "    y_train_T, y_val_T = y_T[train_index], y_T[val_index]\n",
    "    \n",
    "    # Reshape\n",
    "    x_train_T = x_train_T.reshape(-1, 1)\n",
    "    x_val_T = x_val_T.reshape(-1, 1)\n",
    "    \n",
    "    # Train the model\n",
    "    estimator_T = MultinomialNB()\n",
    "    param_grid_T = {'alpha': np.arange(0.001, 1, 0.001), 'fit_prior': [True, False]}\n",
    "    grid_search_T = GridSearchCV(estimator=estimator_T, param_grid=param_grid_T, cv=10, n_jobs=-1)\n",
    "    grid_search_T.fit(x_train_T, y_train_T)\n",
    "    best_estimator_T = grid_search_T.best_estimator_\n",
    "    best_estimator_T.fit(x_train_T, y_train_T)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score_T = best_estimator_T.score(x_val_T, y_val_T)\n",
    "    scoreList_T.append(score_T)\n",
    "\n",
    "print('Mean accuracy:', np.mean(scoreList_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc04ad-6575-4960-ac1d-7881be1950f5",
   "metadata": {},
   "source": [
    "#### 2.3.2 Predict the probability of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b7acdd3-a412-4b9f-a81f-051bf3d98036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "x_test_T = x_test_T.reshape(-1, 1)\n",
    "# x_test_T = cv_T.transform(x_test_T)\n",
    "y_test_T = best_estimator_T.predict(x_test_T)\n",
    "proba_test_T = best_estimator_T.predict_proba(x_test_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9e98a-f1d9-4ce9-ae88-e1d10657af03",
   "metadata": {},
   "source": [
    "#### 2.3.3 Combine the results of 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb45835e-858a-490f-82f2-1e78b427b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_R_N = (proba_test_R*proba_test_N)/[312/2873, 1779/2873, 782/2873]\n",
    "p_R_T = (proba_test_R*proba_test_T)/[312/2873, 1779/2873, 782/2873]\n",
    "p_N_T = (proba_test_N*proba_test_T)/[312/2873, 1779/2873, 782/2873]\n",
    "p_R_N_T = (p_R_N*p_R_T*p_N_T)/[312/2873, 1779/2873, 782/2873]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78c81c5b-cef8-4f8d-9e73-520d4f3d4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_R_N_T = []\n",
    "for i in range(len(p_R_N_T)):\n",
    "    x = np.argmax(p_R_N_T[i])\n",
    "    if x == 0:\n",
    "        x = 'Nightlife'\n",
    "    elif x == 1:\n",
    "        x = 'Restaurants'\n",
    "    else:\n",
    "        x = 'Shopping'\n",
    "    l_R_N_T.append(x)\n",
    "l_R_N_T = np.array(l_R_N_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214db0b4-c902-4ba2-8550-e4e4e2e96662",
   "metadata": {},
   "source": [
    "#### 2.3.4 Output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d9b69f7-8b58-472a-ad27-af6fa56245c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "result_R_N_T = zip((test['ID']), l_R_N_T)\n",
    "output_R_N_T = pd.DataFrame(data=result_R_N_T, columns=['ID', 'category'])\n",
    "output_R_N_T.to_csv('predict_R_N_T.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6ce81-9d7d-40a2-a8ca-11889dfc3f7c",
   "metadata": {},
   "source": [
    "#### 2.3.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f7843-a0e9-45a2-b4f1-2073d720636a",
   "metadata": {},
   "source": [
    "***We train the model only based on the \"time\" attribute with cross-validation, and the validation accuracy is very low (0.61922).</br>\n",
    "After we combine 'predict_R_N.csv', the accuracy of the prediction results is not changed.</br>\n",
    "We submit the prediction result 'predict_R_N_T.csv' to Kaggle and get a Public Score of 0.90462, same like 'predict_R_N.csv', higher than the baseline of 0.87283.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
